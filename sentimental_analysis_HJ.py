# -*- coding: utf-8 -*-
"""Sentimental_Analysis-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lOwXRO8UQgsY7YLJkf9gMAMODpYzpZO5

Sentimental Analysis using Bert
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import re
import nltk
import json
from tqdm import tqdm
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import seaborn as sns
import string
from tqdm import tqdm
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split

datasets = pd.read_json("/content/drive/MyDrive/Colab_Notebooks/Amazon_Instant_Video_5.json", lines=True, orient='records')
datasets.head()

print(len(datasets))
datasets.drop_duplicates(subset=['reviewText'], inplace = True)
print(len(datasets))

datasets = datasets.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'summary', 'unixReviewTime', 'reviewTime'], axis=1)
datasets.head()

sns.countplot(datasets['overall'])

def labelling(x): # negative = 0, positive = 1
  if x<4:
    return 0
  elif x>4:
    return 2
  else:
    return 1

datasets['overall'] = pd.to_numeric(datasets['overall'])
datasets['overall'] = datasets['overall'].apply(labelling)
datasets

sns.countplot(datasets['overall'])

def decontractions(phrase):
    '''Performs decontractions in the doc'''

    phrase = re.sub(r"won't", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)
    phrase = re.sub(r"couldn\'t", "could not", phrase)
    phrase = re.sub(r"shouldn\'t", "should not", phrase)
    phrase = re.sub(r"wouldn\'t", "would not", phrase)
    # general
    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
        
    return phrase

def preprocess(text):
    
    punctuations = set(string.punctuation)
    stop_words = stopwords.words('english')
    new_text = []
    new_score = []
    for line, score in text.values:
        if len(line.split()) == 1 and line in stop_words:
            continue
        else:
            # lowercase
            line = line.lower()
            # removes punctuations
            temp = []
            for i in line:
                if i in punctuations or i.isdigit():
                    continue
                else:
                    temp.append(i)
                line = ''.join(e for e in temp)
            line = re.sub('<.*?>', ' ', line)
            # performs decontractions
            line = decontractions(line)
            # removes multiple spaces
            line = re.sub(' +', ' ', line)
            
            temp = []
            for word in line.split():
                if len(word) < 2:
                    continue
                else:
                    temp.append(word)
            line = ' '.join(e for e in temp)
            
            new_text.append(line)
            new_score.append(score)
            
    
    datasets = pd.DataFrame()
    datasets['reviewText'] = new_text
    datasets['overall'] = new_score
    
    datasets.to_json('Amazon_Instant_Video_preprocessed.json')
    return datasets

nltk.download('stopwords')
datasets = preprocess(datasets)

datasets.shape

datasets.head()

pip install transformers

from transformers import BertTokenizer, TFBertModel
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

X_train, X_test, y_train, y_test = train_test_split(datasets['reviewText'], datasets['overall'], test_size = 0.2, random_state=101, 
                                                    stratify = datasets['overall'])

y_train.value_counts(), y_test.value_counts()

# length_train = []
# for i in X_train:
#     length_train.append(len(i.split()))
length_train = []

for i in X_train:
  tokens = bert_tokenizer.encode(i, max_length=510, truncation=True)
  length_train.append(len(tokens))

sns.scatterplot(range(X_train.shape[0]), length_train)

max_length = max(length_train) + 2 # for cls and sep tokens
max_length

plt.figure(figsize=(10,5))
plt.subplot(121)
plt.title('y_train')
sns.countplot(y_train)
plt.subplot(122)
plt.title('y_test')
sns.countplot(y_test)

def tokenization(X, max_length):
    X_mask = np.zeros((X.shape[0], max_length))
    X_segment = np.zeros((X.shape[0], max_length))
    X_new = []
    for i in range(len(X.values)):
        temp = bert_tokenizer.encode(X.values[i], pad_to_max_length = True, max_length = max_length, truncation=True)
        X_new.append(temp)
        for j in range(max_length):
            if temp[j] != 0:
                X_mask[i][j] = 1
            else:
                continue
        
    return np.array(X_new), X_mask, X_segment

X_train_tok, X_train_mask, X_train_segment = tokenization(X_train, max_length)
X_test_tok, X_test_mask, X_test_segment = tokenization(X_test, max_length)

import pickle

pickle.dump((X_train, X_train_tok, X_train_mask, X_train_segment, y_train),open('train_data.pkl','wb'))
pickle.dump((X_test, X_test_tok, X_test_mask, X_test_segment, y_test),open('test_data.pkl','wb'))

import tensorflow_hub as hub
from tensorflow.keras.models import Model

tf.keras.backend.clear_session()

input_word_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name="input_word_ids")

input_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name="input_mask")

segment_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name="segment_ids")
 
bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1", trainable=False)
pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])

bert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=pooled_output)

X_train_output = bert_model.predict([X_train_tok, X_train_mask, X_train_segment])
X_test_output = bert_model.predict([X_test_tok, X_test_mask, X_test_segment])

for i in tqdm(range(1, 100)):  
  X_train_output = bert_model.predict([X_train_tok, X_train_mask, X_train_segment])
  X_test_output = bert_model.predict([X_test_tok, X_test_mask, X_test_segment])

pickle.dump((X_train_output, X_test_output), open('Text_Features_from_BERT.pkl','wb'))
y_train_OHE = tf.keras.utils.to_categorical(y_train, 3)
y_test_OHE = tf.keras.utils.to_categorical(y_test, 3)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(n_jobs=-1)
model_lr = lr.fit(X_train_output, y_train)

lr.predict(X_test_output[9].reshape(1,-1))

lr_predictions = lr.predict(X_test_output)

from sklearn.metrics import roc_auc_score, confusion_matrix
plt.title('Confusion Matrix')
sns.heatmap(confusion_matrix(y_test.values, lr_predictions), cmap='Reds', annot=True)
plt.xlabel('Predicted')
plt.ylabel('Actual')

class scoring(tf.keras.callbacks.Callback):
    
    def __init__(self, validation_data):
        self.validation_data = validation_data
    def on_train_begin(self, logs={}):
        self.auc=0
    def on_epoch_end(self, epoch, logs={}):
        
        predictions = self.model.predict(self.validation_data[0])
        y_val = self.validation_data[1]
     #   pred = []
        print(predictions.shape)
    #    for i in range(predictions.shape[0]):
    #       pred.append(np.argmax(predictions[i]))
        
        self.auc = roc_auc_score(y_val, predictions, multi_class='ovr')
        
        print("AUC: {}".format(self.auc))

from tensorflow.keras.layers import Input, Dense, Dropout
input_layer = Input(shape=(768,), name='Input_layer')
dense1 = Dense(256, activation = 'relu', kernel_initializer=tf.keras.initializers.he_normal(), name='Dense1')(input_layer)
dropout1 = Dropout(0.5)(dense1)
dense2 = Dense(256, activation = 'relu', kernel_initializer=tf.keras.initializers.he_normal(), name='Dense2')(dropout1)
dropout2 = Dropout(0.5)(dense2)
output_layer = Dense(3, activation='softmax', name='output_layer')(dropout2)

model = Model(inputs=input_layer, outputs=output_layer)
model.summary()

import datetime

optimizer = tf.keras.optimizers.Adam(0.0001)
model.compile(optimizer, 'categorical_crossentropy', metrics=['accuracy'])
log_dir="Tensorboard\logs_m2\\fit4\\" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)

checkpoint = tf.keras.callbacks.ModelCheckpoint('Weights/Model_logsm2_fit4_best.h5', verbose=1, save_best_only=True)
auc = scoring((X_test_output, y_test))

model.fit(X_train_output, y_train_OHE, epochs = 10, validation_data=(X_test_output, y_test_OHE), batch_size = 32,
          callbacks=[tensorboard_callback, checkpoint, auc], verbose = 1)